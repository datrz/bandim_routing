{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe8Nh9Z/hVixiymTl7G2UV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datrz/bandim_routing/blob/main/Recap_Notes_on_Regulatization_and_Decision_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recap Notes on Regulatization and Decision Trees\n",
        "## Explain me L1 norm in Regularization of ML models:\n",
        "\n",
        "L1 norm, also known as the Manhattan norm or the least absolute deviation, is a regularization technique used in machine learning models to reduce the complexity of a model by limiting the number of features used. It works by adding a penalty to the cost function of the model based on the sum of the absolute values of the weights of each feature. The higher the value of the penalty, the less complex the model. By adding the L1 norm penalty, unnecessary features that have little or no impact on the model's performance are eliminated, resulting in a simpler, more effective model.\n",
        "\n",
        "## Explain me L2 norm in Regularization of ML models:\n",
        "\n",
        "L2 norm, also known as the Euclidean norm or the least squares penalty, is a regularization technique used in machine learning models to reduce the complexity of a model by limiting the number of features used. It works by adding a penalty to the cost function of the model based on the sum of the squares of the weights of each feature. The higher the value of the penalty, the less complex the model. By adding the L2 norm penalty, unnecessary features that have little or no impact on the model's performance are eliminated, resulting in a simpler, more effective model.\n",
        "\n",
        "## What is the difference between L1 and L2 norms for Regulatization?\n",
        "\n",
        "The primary difference between L1 and L2 norms is the shape of the penalty they impose on the cost function of a model. L1 norm penalizes the model based on the sum of the absolute values of the weights of each feature, while L2 norm penalizes the model based on the sum of the squares of the weights of each feature. Additionally, L1 norm tends to produce sparser models than L2 norm, which can be beneficial when dealing with high-dimensional data.\n",
        "\n",
        "## Why do we use Regularization when building ML models?\n",
        "\n",
        "Regularization is used when building machine learning models to reduce the complexity of the model and improve its generalization performance. By adding a penalty to the cost function of the model, regularization techniques such as L1 and L2 norms help to eliminate unnecessary features that have little or no impact on the model's performance, resulting in a simpler, more effective model. Regularization also helps to prevent overfitting by constraining the model from adapting too closely to the training data.\n",
        "\n",
        "## What is the best Regularization technique for ML models?\n",
        "\n",
        "There is no single best regularization technique for machine learning models. The best regularization technique for a particular model will depend on the data being used and the specific goals of the model. Generally speaking, L1 norm tends to produce sparser models than L2 norm, which can be beneficial when dealing with high-dimensional data. That said, it is always best to experiment with both techniques and choose the one that performs best for a given model.\n",
        "\n",
        "## How can Regularization be applied for Decision Trees?\n",
        "\n",
        "Regularization can be applied to decision trees in a number of ways. One common technique is to limit the depth of the tree, which helps to prevent overfitting by limiting the number of branches that can be created in the tree. Another technique is to limit the number of nodes in the tree, which helps to reduce the complexity of the tree and improve its generalization performance. Finally, pruning can be used to remove branches of the tree that are not adding any value to the model.\n",
        "\n",
        "## Can L1 and L2 regularization be applied on Decision Trees?\n",
        "\n",
        "No, L1 and L2 regularization techniques are typically only applied to linear models, such as linear regression. They cannot be applied directly to decision trees, as decision trees are non-linear models. However, regularization techniques such as limiting the depth of the tree, limiting the number of nodes, and pruning can still be used to regularize decision trees.\n",
        "\n",
        "## What is the difference between Regression Tree and Classification Tree?\n",
        "\n",
        "The main difference between a regression tree and a classification tree is the type of output that each produces. A regression tree produces a continuous output, while a classification tree produces a discrete output. Additionally, regression trees are used to predict a numerical value, whereas classification trees are used to predict a categorical value. Finally, the splitting criteria used to create the tree is different for each type of tree. For regression trees, the splitting criteria is the sum of squared errors, while for classification trees, the splitting criteria is the Gini index or entropy.\n",
        "\n",
        "## What is GINI Index?\n",
        "\n",
        "The Gini index is a measure of impurity used in classification trees to determine the best split for a node. It is calculated by subtracting the sum of the squared proportions of each class from 1. The lower the Gini index, the purer the node. When used in classification trees, the node with the lowest Gini index is selected as the best split.\n",
        "\n",
        "## What is GINI Index formula for Decision Trees?\n",
        "\n",
        "The Gini index formula for decision trees is as follows:\n",
        "\n",
        "$Gini\\ index = 1 – ∑(pk^2)$\n",
        "\n",
        "where $pk$ is the proportion of the $k$th class in the node.\n",
        "\n",
        "## What is Entropy for Decision Trees?\n",
        "\n",
        "Entropy is a measure of impurity used in classification trees to determine the best split for a node. It is calculated by summing the negative of the proportion of each class multiplied by the logarithm of the proportion. The higher the entropy, the more impure the node. When used in classification trees, the node with the highest entropy is selected as the best split.\n",
        "\n",
        "## What is the formula for Entropy for Decision Trees?\n",
        "\n",
        "The formula for entropy for decision trees is as follows:\n",
        "\n",
        "$Entropy = -∑(pk * log(pk))$\n",
        "\n",
        "where $pk$ is the proportion of the $k$th class in the node.\n",
        "\n",
        "## What is the best measure to evaluate a Decision Tree Machine Learning model?\n",
        "\n",
        "The best measure to evaluate a decision tree machine learning model depends on the goal of the model. Generally speaking, accuracy or F1 score are good metrics to use when evaluating a classification tree, while mean squared error (MSE) is a good metric to use when evaluating a regression tree. Additionally, metrics such as precision, recall, and area under the curve (AUC) can also be used to evaluate the performance of a classification tree.\n",
        "\n",
        "## What is Mean Square Error?\n",
        "\n",
        "Mean squared error (MSE) is a measure of how close a machine learning model is to the true values it is trying to predict. It is calculated by taking the average of the squared differences between the predicted values and the true values. The lower the MSE, the better the model's performance. MSE is commonly used to evaluate the performance of regression models.\n",
        "\n",
        "## What is F1 for Decision Trees?\n",
        "\n",
        "F1 score is a metric used to evaluate the performance of a classification tree. It combines precision and recall into a single metric and is calculated by taking the harmonic mean of the two values. The F1 score ranges from 0 to 1, with 1 being the best possible score. A higher F1 score indicates that the model is accurately predicting the true classes of the data.\n",
        "\n",
        "## What is the formula of F1 for Decision Trees?\n",
        "\n",
        "The formula for the F1 score is as follows:\n",
        "\n",
        "$F1 = 2 * (precision * recall)/(precision + recall)$\n",
        "\n",
        "## What is the formula for accuracy for Decision Trees?\n",
        "\n",
        "The formula for accuracy for decision trees is as follows:\n",
        "\n",
        "Accuracy = (number of correctly classified instances)/(total number of instances)"
      ],
      "metadata": {
        "id": "3lqL88ucphp9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU6jSpSjpg3x"
      },
      "outputs": [],
      "source": []
    }
  ]
}